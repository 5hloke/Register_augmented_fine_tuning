{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcd349b-d6b0-41d3-bfca-27021aec66ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from model import RegBertForQA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387bacf2-da1c-4f64-a57e-b8c18625f2d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from regbertfor QA, num_reg= 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RegBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_ids', 'bert.embeddings.reg_pos', 'bert.embeddings.reg_tokens', 'bert.reg_pos', 'bert.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RegBertForQA were not initialized from the model checkpoint at fine_tuned_model_registers_Nov17 and are newly initialized: ['bert.embeddings.reg_pos', 'bert.embeddings.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1, Sequence length: 71, Num registers: 50\n",
      "Original token_type_ids shape: torch.Size([50, 121])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected token_type_ids shape (1, 71), but got torch.Size([50, 121])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m start_logit \u001b[38;5;241m+\u001b[39m end_logit \n\u001b[1;32m     74\u001b[0m ig \u001b[38;5;241m=\u001b[39m IntegratedGradients(forward_func)\n\u001b[0;32m---> 76\u001b[0m attributions, delta \u001b[38;5;241m=\u001b[39m \u001b[43mig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds_with_registers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattention_mask_with_registers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids_with_registers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m attributions_sum \u001b[38;5;241m=\u001b[39m attributions\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \n\u001b[1;32m     85\u001b[0m register_token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[REG\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_registers)]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[38;5;241m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(step_sizes)\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(grad\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/captum/_utils/common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mforward_func\u001b[0;34m(inputs_embeds_with_registers, attention_mask_with_registers, token_type_ids_with_registers)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_func\u001b[39m(inputs_embeds_with_registers, attention_mask_with_registers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, token_type_ids_with_registers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 53\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds_with_registers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_with_registers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids_with_registers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     start_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mstart_logits  \n\u001b[1;32m     60\u001b[0m     end_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mend_logits     \n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/LLM/Project/model.py:277\u001b[0m, in \u001b[0;36mRegBertForQA.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 277\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# print('outputs[0]',outputs[0].shape)\u001b[39;00m\n\u001b[1;32m    291\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/LLM/Project/model.py:146\u001b[0m, in \u001b[0;36mRegBert.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    141\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    142\u001b[0m         (batch_size, seq_length), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Ensure token_type_ids matches input_ids shape\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m token_type_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m token_type_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m seq_length, \\\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected token_type_ids shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(batch_size,\u001b[38;5;250m \u001b[39mseq_length)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_type_ids\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Create register token type IDs\u001b[39;00m\n\u001b[1;32m    150\u001b[0m register_token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    151\u001b[0m     (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_registers), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev\n\u001b[1;32m    152\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected token_type_ids shape (1, 71), but got torch.Size([50, 121])"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('fine_tuned_model_registers_Nov17')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('fine_tuned_model_orig')\n",
    "\n",
    "model_path = 'fine_tuned_model_registers_Nov17'  # Replace with your actual model path\n",
    "# model_path = 'fine_tuned_model_orig'  # Replace with your actual model path\n",
    "\n",
    "model = RegBertForQA.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "\"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(device)            \n",
    "token_type_ids = inputs['token_type_ids'].to(device)  \n",
    "attention_mask = inputs['attention_mask'].to(device)  \n",
    "\n",
    "batch_size = input_ids.shape[0]\n",
    "seq_length = input_ids.shape[1]\n",
    "num_registers = model.bert.num_registers\n",
    "\n",
    "inputs_embeds = model.bert.embeddings(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=token_type_ids\n",
    ")  \n",
    "\n",
    "register_tokens = model.bert.reg_tokens.expand(batch_size, -1, -1).to(device)\n",
    "register_pos = model.bert.reg_pos.to(device)\n",
    "register_embeddings = register_tokens + register_pos  \n",
    "\n",
    "inputs_embeds_with_registers = torch.cat((register_embeddings, inputs_embeds), dim=1)  # Shape: [batch_size, num_registers + seq_length, embedding_dim]\n",
    "inputs_embeds_with_registers.requires_grad_()\n",
    "\n",
    "register_attention_mask = torch.ones((batch_size, num_registers), device=device)\n",
    "attention_mask_with_registers = torch.cat((register_attention_mask, attention_mask), dim=1)  # Shape: [batch_size, num_registers + seq_length]\n",
    "\n",
    "register_token_type_ids = torch.zeros((batch_size, num_registers), dtype=token_type_ids.dtype, device=device)\n",
    "token_type_ids_with_registers = torch.cat((register_token_type_ids, token_type_ids), dim=1)  # Shape: [batch_size, num_registers + seq_length]\n",
    "\n",
    "baseline_embeds = torch.zeros_like(inputs_embeds_with_registers, device=device)\n",
    "\n",
    "def forward_func(inputs_embeds_with_registers, attention_mask_with_registers=None, token_type_ids_with_registers=None):\n",
    "    outputs = model(\n",
    "        inputs_embeds=inputs_embeds_with_registers,\n",
    "        attention_mask=attention_mask_with_registers,\n",
    "        token_type_ids=token_type_ids_with_registers,\n",
    "        input_ids=input_ids,  \n",
    "    )\n",
    "    start_logits = outputs.start_logits  \n",
    "    end_logits = outputs.end_logits     \n",
    "\n",
    "    start_probs = torch.softmax(start_logits, dim=-1)\n",
    "    end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "    start_index = torch.argmax(start_probs, dim=-1) \n",
    "    end_index = torch.argmax(end_probs, dim=-1)     \n",
    "    batch_size = start_logits.shape[0]\n",
    "\n",
    "    start_logit = start_logits[torch.arange(batch_size), start_index]  \n",
    "    end_logit = end_logits[torch.arange(batch_size), end_index]        \n",
    "\n",
    "    return start_logit + end_logit \n",
    "\n",
    "ig = IntegratedGradients(forward_func)\n",
    "\n",
    "attributions, delta = ig.attribute(\n",
    "    inputs_embeds_with_registers,\n",
    "    baselines=baseline_embeds,\n",
    "    additional_forward_args=(attention_mask_with_registers, token_type_ids_with_registers),\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "attributions_sum = attributions.sum(dim=-1)[0]  \n",
    "\n",
    "register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "\n",
    "all_tokens =  register_token_ids + input_tokens\n",
    "\n",
    "print(\"Number of tokens:\", len(all_tokens))\n",
    "print(\"Number of attributions:\", len(attributions_sum))\n",
    "\n",
    "for token, attribution in zip(all_tokens, attributions_sum):\n",
    "    print(f'{token}: {attribution.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b4746-e298-4bab-b0ce-70e0b550cc73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "attributions_np = attributions_sum.detach().cpu().numpy()\n",
    "attributions_norm = attributions_np / np.linalg.norm(attributions_np)\n",
    "\n",
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds_with_registers,\n",
    "    attention_mask=attention_mask_with_registers,\n",
    "    token_type_ids=token_type_ids_with_registers,\n",
    "    input_ids=None,\n",
    ")\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "pred_prob = ((start_probs[0, start_index] + end_probs[0, end_index]) / 2).item()\n",
    "\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "vis_data_record = viz.VisualizationDataRecord(\n",
    "    word_attributions=attributions_norm,\n",
    "    pred_prob=pred_prob,\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  \n",
    "    attr_class=\"\",\n",
    "    attr_score=attributions_norm.sum(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=delta.item()\n",
    ")\n",
    "\n",
    "# Visualize attributions\n",
    "viz.visualize_text([vis_data_record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e227899-8e32-4dce-a171-fe5a8d12dfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb81cf2-3c34-42ce-b76f-a0d071a544f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eb689-995f-4d58-abfc-2d6f22f92b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed0add-2b7c-4da5-87a4-f6afa36fe39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a5e1b-eb4b-4f47-b8d5-aba3db8af19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd1305-8fbc-429d-978f-9f898b9bb0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670e932-436a-4f08-aa50-cddf5854511b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Obtain attention weights from the model\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "attentions = outputs.attentions  # A tuple of attention matrices\n",
    "\n",
    "# Visualize attention for a specific layer and head\n",
    "import seaborn as sns\n",
    "\n",
    "layer = 0  # Choose the layer to analyze\n",
    "head = 0   # Choose the head\n",
    "\n",
    "attention_weights = attentions[layer][0][head].detach().cpu().numpy()\n",
    "sns.heatmap(attention_weights)\n",
    "plt.title(f'Attention Weights - Layer {layer+1}, Head {head+1}')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Tokens')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335ffd6-77ac-46d4-bbb6-88c96bc61138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8b230-a471-4e13-a44c-67b20a00a017",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from captum.attr import IntegratedGradients\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('fine_tuned_model')\n",
    "\n",
    "model_path = 'fine_tuned_model'  \n",
    "model = RegBertForQA.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "question, context = (\n",
    "    \"How long is the Great Wall of China?\",\n",
    "    \"The Great Wall of China is an ancient wall in China. The wall was built to protect the northern borders of the Chinese Empire from invading nomadic tribes. The Great Wall stretches from Dandong in the east to Lop Lake in the west, covering about 13,000 miles.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "token_type_ids = inputs['token_type_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "# Prepare inputs_embeds with register tokens\n",
    "batch_size = input_ids.shape[0]\n",
    "seq_length = input_ids.shape[1]\n",
    "num_registers = model.bert.num_registers\n",
    "\n",
    "inputs_embeds = model.bert.embeddings(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=token_type_ids\n",
    ")\n",
    "\n",
    "register_tokens = model.bert.reg_tokens.expand(batch_size, -1, -1).to(device)\n",
    "register_pos = model.bert.reg_pos.to(device)\n",
    "register_embeddings = register_tokens + register_pos\n",
    "\n",
    "inputs_embeds_with_registers = torch.cat((register_embeddings, inputs_embeds), dim=1)  # Shape: [batch_size, num_registers + seq_length, embedding_dim]\n",
    "inputs_embeds_with_registers.requires_grad_()\n",
    "\n",
    "# Extend attention masks\n",
    "register_attention_mask = torch.ones((batch_size, num_registers), device=device)\n",
    "attention_mask_with_registers = torch.cat((register_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "register_token_type_ids = torch.zeros((batch_size, num_registers), dtype=token_type_ids.dtype, device=device)\n",
    "token_type_ids_with_registers = torch.cat((register_token_type_ids, token_type_ids), dim=1)\n",
    "\n",
    "# Baseline embeddings (zeros or padding embeddings)\n",
    "baseline_embeds = torch.zeros_like(inputs_embeds_with_registers, device=device)\n",
    "\n",
    "# Define forward function\n",
    "def forward_func(inputs_embeds_with_registers, attention_mask_with_registers=None, token_type_ids_with_registers=None, target=None):\n",
    "    outputs = model(\n",
    "        inputs_embeds=inputs_embeds_with_registers,\n",
    "        attention_mask=attention_mask_with_registers,\n",
    "        token_type_ids=token_type_ids_with_registers,\n",
    "        input_ids=None,  # Not passing input_ids since embeddings are provided\n",
    "    )\n",
    "    start_logits = outputs.start_logits  # Shape: [batch_size, seq_length]\n",
    "    end_logits = outputs.end_logits      # Shape: [batch_size, seq_length]\n",
    "    \n",
    "    # Return logits for the specific target index\n",
    "    if target is not None:\n",
    "        return start_logits[:, target] if target == 0 else end_logits[:, target]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid target value. Must be 0 (start logits) or 1 (end logits).\")\n",
    "\n",
    "# Define Integrated Gradients\n",
    "ig = IntegratedGradients(forward_func)\n",
    "\n",
    "# Get predicted start and end indices\n",
    "start_index = torch.argmax(model(input_ids=input_ids).start_logits, dim=-1)\n",
    "end_index = torch.argmax(model(input_ids=input_ids).end_logits, dim=-1)\n",
    "\n",
    "# Compute attributions for start logits\n",
    "start_attributions, start_delta = ig.attribute(\n",
    "    inputs_embeds_with_registers,\n",
    "    baselines=baseline_embeds,\n",
    "    additional_forward_args=(attention_mask_with_registers, token_type_ids_with_registers, start_index.item()),  # Target = predicted start index\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Compute attributions for end logits\n",
    "end_attributions, end_delta = ig.attribute(\n",
    "    inputs_embeds_with_registers,\n",
    "    baselines=baseline_embeds,\n",
    "    additional_forward_args=(attention_mask_with_registers, token_type_ids_with_registers, end_index.item()),  # Target = predicted end index\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum attributions across embedding dimensions\n",
    "start_attributions_sum = start_attributions.sum(dim=-1)[0]\n",
    "end_attributions_sum = end_attributions.sum(dim=-1)[0]\n",
    "\n",
    "# Combine tokens (register + input tokens)\n",
    "register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "all_tokens = register_token_ids + input_tokens\n",
    "\n",
    "# Print attributions\n",
    "print(\"Token-Level Attributions for Start Logits:\")\n",
    "for token, attribution in zip(all_tokens, start_attributions_sum):\n",
    "    print(f\"{token}: {attribution.item()}\")\n",
    "\n",
    "print(\"\\nToken-Level Attributions for End Logits:\")\n",
    "for token, attribution in zip(all_tokens, end_attributions_sum):\n",
    "    print(f\"{token}: {attribution.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44835c9-57c1-4411-b298-5f0d9a2cf818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Convert attributions to numpy and normalize\n",
    "start_attributions_np = start_attributions_sum.detach().cpu().numpy()\n",
    "end_attributions_np = end_attributions_sum.detach().cpu().numpy()\n",
    "\n",
    "start_attributions_norm = start_attributions_np / np.linalg.norm(start_attributions_np)\n",
    "end_attributions_norm = end_attributions_np / np.linalg.norm(end_attributions_np)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds_with_registers,\n",
    "    attention_mask=attention_mask_with_registers,\n",
    "    token_type_ids=token_type_ids_with_registers,\n",
    "    input_ids=None,\n",
    ")\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "# Adjust indices for register tokens\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "# Get predicted answer\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "# Visualize attributions for start and end logits\n",
    "start_vis_data_record = viz.VisualizationDataRecord(\n",
    "    word_attributions=start_attributions_norm,\n",
    "    pred_prob=start_probs[0, start_index].item(),\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"Start Logits\",\n",
    "    attr_score=start_attributions_norm.sum(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=start_delta.item()\n",
    ")\n",
    "\n",
    "end_vis_data_record = viz.VisualizationDataRecord(\n",
    "    word_attributions=end_attributions_norm,\n",
    "    pred_prob=end_probs[0, end_index].item(),\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"End Logits\",\n",
    "    attr_score=end_attributions_norm.sum(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=end_delta.item()\n",
    ")\n",
    "\n",
    "# Visualize both start and end attributions\n",
    "viz.visualize_text([start_vis_data_record, end_vis_data_record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39102053-7b83-449e-b137-3c60c0606471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3152787-4127-4723-8064-91b57bd6e786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551c3f5-fc6e-4dee-8999-60516a698212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
