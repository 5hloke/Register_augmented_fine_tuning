{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e205c688-c945-42be-bcf8-f8fed7661190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from regbertfor QA, num_reg= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RegBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_ids', 'bert.reg_pos', 'bert.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RegBertForQA were not initialized from the model checkpoint at fine_tuned_model_orig and are newly initialized: ['bert.embeddings.position_ids', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.reg_pos', 'bert.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: torch.Size([1, 434])\n",
      "Position_ids: tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "         504, 505, 506, 507, 508, 509, 510, 511]], device='cuda:0')\n",
      "torch.Size([1, 434, 768]) torch.Size([1, 434, 768])\n",
      "outputs: torch.Size([1, 434, 768])\n",
      "Start logits shape: torch.Size([1, 434])\n",
      "End logits shape: torch.Size([1, 434])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashlok/Register_augmented_fine_tuning/ExplanationGenerator.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.model.relprop(torch.tensor(one_hot).to(input_ids.device), **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from model import RegBertForQA  \n",
    "# from ExplanationGenerator import Generator \n",
    "# from captum.attr import visualization\n",
    "# from BERT import BertModel\n",
    "# import torch\n",
    "\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\").to(\"cuda\")\n",
    "\n",
    "# # model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "# model.eval()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_orig\")\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "# explanations = Generator(model)\n",
    "\n",
    "# question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "# \"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# # context = \"Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1\"\n",
    "# # question = \"When was quantum field theory developed?\"\n",
    "# # num_registers = 50\n",
    "# # register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# # input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# # all_tokens =  register_token_ids + input_tokens\n",
    "# encoding = tokenizer(\n",
    "#     question,\n",
    "#     context,\n",
    "#     return_tensors=\"pt\",\n",
    "#     # padding=\"max_length\",\n",
    "#     # truncation=True,\n",
    "#     max_length=434\n",
    "# )\n",
    "# input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "# # token_type_ids = encoding['token_type_ids'].to(\"cuda\")  \n",
    "# attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "# expl = explanations.generate_LRP(\n",
    "#     input_ids=input_ids, \n",
    "#     attention_mask=attention_mask, \n",
    "#     # start_positions=None,  \n",
    "#     # end_positions=None,   \n",
    "#     start_layer=0\n",
    "# )[0]\n",
    "from transformers import AutoTokenizer\n",
    "from model import RegBertForQA  \n",
    "from ExplanationGenerator import Generator \n",
    "from captum.attr import visualization\n",
    "from BERT import BertModel\n",
    "import torch\n",
    "\n",
    "model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\", num_registers = 0).to(\"cuda\")\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_orig\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "explanations = Generator(model)\n",
    "\n",
    "question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "\"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# num_registers = 50\n",
    "# register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# all_tokens =  register_token_ids + input_tokens\n",
    "encoding = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=434\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "expl = explanations.generate_LRP(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    # start_positions=None,  \n",
    "    # end_positions=None,   \n",
    "    start_layer=0\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ca071-6bce-4513-bd70-c87feabcb730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjusted_start_index, adjusted_end_index\n",
    "# ! pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717b89c-0355-4bbe-b594-cb5acc3b5980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# get the model classification\n",
    "# print(model._logits[0], len(model._logits))\n",
    "# output = torch.nn.functional.softmax(model._logits[0], dim=-1)\n",
    "# print(output.shape)\n",
    "# classification = output.argmax(dim=-1).item()\n",
    "# # get class name\n",
    "# class_name = classifications[classification]\n",
    "# # if the classification is negative, higher explanation scores are more negative\n",
    "# # flip for visualization\n",
    "# if class_name == \"NEGATIVE\":\n",
    "#   expl *= (-1)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "# print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "# vis_data_records = [visualization.VisualizationDataRecord(\n",
    "#                                 expl,\n",
    "#                                 output[0][classification],\n",
    "#                                 classification,\n",
    "#                                 true_class,\n",
    "#                                 true_class,\n",
    "#                                 1,       \n",
    "#                                 tokens,\n",
    "#                                 1)]\n",
    "# visualization.visualize_text(vis_data_records)\n",
    "outputs = model(\n",
    "    input_ids=input_ids, attention_mask=attention_mask\n",
    ")\n",
    "num_registers = model.bert.num_registers\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "pred_prob = ((start_probs[0, start_index] + end_probs[0, end_index]) / 2).item()\n",
    "\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "\n",
    "vis_data_record = visualization.VisualizationDataRecord(\n",
    "    word_attributions=expl,\n",
    "    pred_prob=pred_prob,\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"\",\n",
    "    attr_score=1,\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=1\n",
    ")\n",
    "\n",
    "# Visualize attributions\n",
    "visualization.visualize_text([vis_data_record])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743015d-30a4-434e-9714-3f8f0e7354f3",
   "metadata": {},
   "source": [
    "### With registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c490a7c-3ca9-4b13-92b0-7b72d81a283a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model import RegBertForQA  \n",
    "from ExplanationGenerator import Generator \n",
    "from captum.attr import visualization\n",
    "from BERT import BertModel\n",
    "import torch\n",
    "\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\").to(\"cuda\")\n",
    "model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"../fine_tuned_model_orig\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "explanations = Generator(model)\n",
    "\n",
    "question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "\"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# num_registers = 50\n",
    "# register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# all_tokens =  register_token_ids + input_tokens\n",
    "encoding = tokenizer.encode_plus(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\",\n",
    "    # padding=\"max_length\",\n",
    "    # truncation=True,\n",
    "    # max_length=434\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "token_type_ids = encoding['token_type_ids'].to(\"cuda\")  \n",
    "attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "expl = explanations.generate_LRP(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    # start_positions=None,  \n",
    "    # end_positions=None,   \n",
    "    start_layer=0\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db077c06-59f9-4d61-822b-9284849f7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids, attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    ")\n",
    "num_registers = model.bert.num_registers\n",
    "print(num_registers)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "pred_prob = ((start_probs[0, start_index] + end_probs[0, end_index]) / 2).item()\n",
    "\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "\n",
    "all_tokens =  register_token_ids + input_tokens\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "\n",
    "vis_data_record = visualization.VisualizationDataRecord(\n",
    "    word_attributions=expl,\n",
    "    pred_prob=pred_prob,\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"\",\n",
    "    attr_score=1,\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=1\n",
    ")\n",
    "print(adjusted_start_index, adjusted_end_index)\n",
    "# Visualize attributions\n",
    "visualization.visualize_text([vis_data_record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c470d-53d0-4f3a-a5a8-e4bdd82444ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf6488-61ce-46c5-9205-5163aed0b936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa20d4-7a32-42f9-9d97-15facf4fc9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2152ee-b72c-43f9-87ef-ae22f429e09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721cfd-edd1-4637-b337-e31dcbac8133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Normalize scores for visualization\n",
    "expl_start = (expl_start - expl_start.min()) / (expl_start.max() - expl_start.min())\n",
    "# expl_end = (expl_end - expl_end.min()) / (expl_end.max() - expl_end.min())\n",
    "\n",
    "# Get the model's predicted start and end indices\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "start_idx = torch.argmax(output.start_logits, dim=-1).item()\n",
    "# end_idx = torch.argmax(output.end_logits, dim=-1).item()\n",
    "\n",
    "# Decode the predicted answer\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "# predicted_answer = tokenizer.decode(input_ids[0][start_idx:end_idx + 1])\n",
    "\n",
    "# Display explanation scores and tokens\n",
    "# print(\"Predicted Answer:\", predicted_answer)\n",
    "print(\"Start Token Explanations:\")\n",
    "print([(tokens[i], expl_start[i].item()) for i in range(len(tokens))])\n",
    "# print(\"End Token Explanations:\")\n",
    "# print([(tokens[i], expl_end[i].item()) for i in range(len(tokens))])\n",
    "\n",
    "# Visualize explanations for start and end\n",
    "vis_data_records_start = [visualization.VisualizationDataRecord(\n",
    "    expl_start,\n",
    "    output.start_logits[0][start_idx].item(),\n",
    "    start_idx,\n",
    "    None,\n",
    "    None,\n",
    "    1,       \n",
    "    tokens,\n",
    "    1\n",
    ")]\n",
    "# vis_data_records_end = [visualization.VisualizationDataRecord(\n",
    "#     expl_end,\n",
    "#     output.end_logits[0][end_idx].item(),\n",
    "#     end_idx,\n",
    "#     None,\n",
    "#     None,\n",
    "#     1,       \n",
    "#     tokens,\n",
    "#     1\n",
    "# )]\n",
    "\n",
    "print(\"\\nVisualizing Start Explanations:\")\n",
    "visualization.visualize_text(vis_data_records_start)\n",
    "\n",
    "# print(\"\\nVisualizing End Explanations:\")\n",
    "# visualization.visualize_text(vis_data_records_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c20eb-4741-490b-8e4f-9f68f3b49327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
