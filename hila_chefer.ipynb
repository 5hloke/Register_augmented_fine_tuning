{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e205c688-c945-42be-bcf8-f8fed7661190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gnilay/.conda/envs/llm/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from regbertfor QA, num_reg= 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RegBert were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_ids', 'bert.reg_pos', 'bert.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RegBertForQA were not initialized from the model checkpoint at fine_tuned_model_orig and are newly initialized: ['bert.embeddings.position_ids', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'bert.reg_pos', 'bert.reg_tokens']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (torch.Size, dev=str, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m expl \u001b[38;5;241m=\u001b[39m \u001b[43mexplanations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_LRP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# start_positions=None,  \u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# end_positions=None,   \u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     84\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/LLM/Project/ExplanationGenerator.py:30\u001b[0m, in \u001b[0;36mGenerator.generate_LRP\u001b[0;34m(self, input_ids, attention_mask, start_index, end_index, start_layer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_LRP\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask,\n\u001b[1;32m     29\u001b[0m                  start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, end_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, start_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     start_logits, end_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m], outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_logits[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/LLM/Project/model.py:246\u001b[0m, in \u001b[0;36mRegBertForQA.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 246\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# print('outputs[0]',outputs[0].shape)\u001b[39;00m\n\u001b[1;32m    260\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/LLM/Project/model.py:134\u001b[0m, in \u001b[0;36mRegBert.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    132\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Here are the positional embeddings + word embeddings + token type embeddings\u001b[39;00m\n\u001b[1;32m    137\u001b[0m input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (torch.Size, dev=str, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from model import RegBertForQA  \n",
    "# from ExplanationGenerator import Generator \n",
    "# from captum.attr import visualization\n",
    "# from BERT import BertModel\n",
    "# import torch\n",
    "\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\").to(\"cuda\")\n",
    "\n",
    "# # model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "# model.eval()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_orig\")\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "# explanations = Generator(model)\n",
    "\n",
    "# question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "# \"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# # context = \"Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1\"\n",
    "# # question = \"When was quantum field theory developed?\"\n",
    "# # num_registers = 50\n",
    "# # register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# # input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# # all_tokens =  register_token_ids + input_tokens\n",
    "# encoding = tokenizer(\n",
    "#     question,\n",
    "#     context,\n",
    "#     return_tensors=\"pt\",\n",
    "#     # padding=\"max_length\",\n",
    "#     # truncation=True,\n",
    "#     max_length=434\n",
    "# )\n",
    "# input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "# # token_type_ids = encoding['token_type_ids'].to(\"cuda\")  \n",
    "# attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "# expl = explanations.generate_LRP(\n",
    "#     input_ids=input_ids, \n",
    "#     attention_mask=attention_mask, \n",
    "#     # start_positions=None,  \n",
    "#     # end_positions=None,   \n",
    "#     start_layer=0\n",
    "# )[0]\n",
    "from transformers import AutoTokenizer\n",
    "from model import RegBertForQA  \n",
    "from ExplanationGenerator import Generator \n",
    "from captum.attr import visualization\n",
    "from BERT import BertModel\n",
    "import torch\n",
    "\n",
    "model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\").to(\"cuda\")\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_orig\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "explanations = Generator(model)\n",
    "\n",
    "question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "\"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# num_registers = 50\n",
    "# register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# all_tokens =  register_token_ids + input_tokens\n",
    "encoding = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=434\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "expl = explanations.generate_LRP(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    # start_positions=None,  \n",
    "    # end_positions=None,   \n",
    "    start_layer=0\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ca071-6bce-4513-bd70-c87feabcb730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjusted_start_index, adjusted_end_index\n",
    "# ! pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717b89c-0355-4bbe-b594-cb5acc3b5980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# get the model classification\n",
    "# print(model._logits[0], len(model._logits))\n",
    "# output = torch.nn.functional.softmax(model._logits[0], dim=-1)\n",
    "# print(output.shape)\n",
    "# classification = output.argmax(dim=-1).item()\n",
    "# # get class name\n",
    "# class_name = classifications[classification]\n",
    "# # if the classification is negative, higher explanation scores are more negative\n",
    "# # flip for visualization\n",
    "# if class_name == \"NEGATIVE\":\n",
    "#   expl *= (-1)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "# print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "# vis_data_records = [visualization.VisualizationDataRecord(\n",
    "#                                 expl,\n",
    "#                                 output[0][classification],\n",
    "#                                 classification,\n",
    "#                                 true_class,\n",
    "#                                 true_class,\n",
    "#                                 1,       \n",
    "#                                 tokens,\n",
    "#                                 1)]\n",
    "# visualization.visualize_text(vis_data_records)\n",
    "outputs = model(\n",
    "    input_ids=input_ids, attention_mask=attention_mask\n",
    ")\n",
    "num_registers = model.bert.num_registers\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "pred_prob = ((start_probs[0, start_index] + end_probs[0, end_index]) / 2).item()\n",
    "\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "\n",
    "vis_data_record = visualization.VisualizationDataRecord(\n",
    "    word_attributions=expl,\n",
    "    pred_prob=pred_prob,\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"\",\n",
    "    attr_score=1,\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=1\n",
    ")\n",
    "\n",
    "# Visualize attributions\n",
    "visualization.visualize_text([vis_data_record])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743015d-30a4-434e-9714-3f8f0e7354f3",
   "metadata": {},
   "source": [
    "### With registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c490a7c-3ca9-4b13-92b0-7b72d81a283a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model import RegBertForQA  \n",
    "from ExplanationGenerator import Generator \n",
    "from captum.attr import visualization\n",
    "from BERT import BertModel\n",
    "import torch\n",
    "\n",
    "# model = RegBertForQA.from_pretrained(\"fine_tuned_model_orig\").to(\"cuda\")\n",
    "model = RegBertForQA.from_pretrained(\"fine_tuned_model_registers_Nov17\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"../fine_tuned_model_orig\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model_registers_Nov17\")\n",
    "explanations = Generator(model)\n",
    "\n",
    "question, context = \"What is a way to increase your wound healing speed?\", \\\n",
    "\"Wound care encourages and speeds wound healing via cleaning and protection from reinjury or infection. Depending on each patient's needs, it can range from the simplest first aid to entire nursing specialties such as wound, ostomy, and continence nursing and burn center care.\"\n",
    "\n",
    "# num_registers = 50\n",
    "# register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "# input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "# all_tokens =  register_token_ids + input_tokens\n",
    "encoding = tokenizer.encode_plus(\n",
    "    question,\n",
    "    context,\n",
    "    return_tensors=\"pt\",\n",
    "    # padding=\"max_length\",\n",
    "    # truncation=True,\n",
    "    # max_length=434\n",
    ")\n",
    "input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "token_type_ids = encoding['token_type_ids'].to(\"cuda\")  \n",
    "attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "expl = explanations.generate_LRP(\n",
    "    input_ids=input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    # start_positions=None,  \n",
    "    # end_positions=None,   \n",
    "    start_layer=0\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db077c06-59f9-4d61-822b-9284849f7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids, attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    ")\n",
    "num_registers = model.bert.num_registers\n",
    "print(num_registers)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_probs = torch.softmax(start_logits, dim=-1)\n",
    "end_probs = torch.softmax(end_logits, dim=-1)\n",
    "\n",
    "start_index = torch.argmax(start_probs, dim=-1).item()\n",
    "end_index = torch.argmax(end_probs, dim=-1).item()\n",
    "\n",
    "adjusted_start_index = max(0, start_index - num_registers)\n",
    "adjusted_end_index = max(0, end_index - num_registers)\n",
    "\n",
    "pred_prob = ((start_probs[0, start_index] + end_probs[0, end_index]) / 2).item()\n",
    "\n",
    "answer_ids = input_ids[0, adjusted_start_index: adjusted_end_index + 1]\n",
    "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "register_token_ids = ['[REG{}]'.format(i) for i in range(num_registers)]\n",
    "\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "\n",
    "all_tokens =  register_token_ids + input_tokens\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "\n",
    "vis_data_record = visualization.VisualizationDataRecord(\n",
    "    word_attributions=expl,\n",
    "    pred_prob=pred_prob,\n",
    "    pred_class=predicted_answer,\n",
    "    true_class=\"\",  # Provide true answer if available\n",
    "    attr_class=\"\",\n",
    "    attr_score=1,\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=1\n",
    ")\n",
    "print(adjusted_start_index, adjusted_end_index)\n",
    "# Visualize attributions\n",
    "visualization.visualize_text([vis_data_record])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c470d-53d0-4f3a-a5a8-e4bdd82444ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf6488-61ce-46c5-9205-5163aed0b936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa20d4-7a32-42f9-9d97-15facf4fc9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2152ee-b72c-43f9-87ef-ae22f429e09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721cfd-edd1-4637-b337-e31dcbac8133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Normalize scores for visualization\n",
    "expl_start = (expl_start - expl_start.min()) / (expl_start.max() - expl_start.min())\n",
    "# expl_end = (expl_end - expl_end.min()) / (expl_end.max() - expl_end.min())\n",
    "\n",
    "# Get the model's predicted start and end indices\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "start_idx = torch.argmax(output.start_logits, dim=-1).item()\n",
    "# end_idx = torch.argmax(output.end_logits, dim=-1).item()\n",
    "\n",
    "# Decode the predicted answer\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "# predicted_answer = tokenizer.decode(input_ids[0][start_idx:end_idx + 1])\n",
    "\n",
    "# Display explanation scores and tokens\n",
    "# print(\"Predicted Answer:\", predicted_answer)\n",
    "print(\"Start Token Explanations:\")\n",
    "print([(tokens[i], expl_start[i].item()) for i in range(len(tokens))])\n",
    "# print(\"End Token Explanations:\")\n",
    "# print([(tokens[i], expl_end[i].item()) for i in range(len(tokens))])\n",
    "\n",
    "# Visualize explanations for start and end\n",
    "vis_data_records_start = [visualization.VisualizationDataRecord(\n",
    "    expl_start,\n",
    "    output.start_logits[0][start_idx].item(),\n",
    "    start_idx,\n",
    "    None,\n",
    "    None,\n",
    "    1,       \n",
    "    tokens,\n",
    "    1\n",
    ")]\n",
    "# vis_data_records_end = [visualization.VisualizationDataRecord(\n",
    "#     expl_end,\n",
    "#     output.end_logits[0][end_idx].item(),\n",
    "#     end_idx,\n",
    "#     None,\n",
    "#     None,\n",
    "#     1,       \n",
    "#     tokens,\n",
    "#     1\n",
    "# )]\n",
    "\n",
    "print(\"\\nVisualizing Start Explanations:\")\n",
    "visualization.visualize_text(vis_data_records_start)\n",
    "\n",
    "# print(\"\\nVisualizing End Explanations:\")\n",
    "# visualization.visualize_text(vis_data_records_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c20eb-4741-490b-8e4f-9f68f3b49327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
