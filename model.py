import torch
import numpy as np 
import torch.nn as nn
# from transformers.models.bert.modeling_bert import BertModel, BertLayer, BertSelfAttention, BaseModelOutputWithPoolingAndCrossAttentions, QuestionAnsweringModelOutput, BertForQuestionAnswering
from BERT import BertModel
from transformers.models.bert.modeling_bert import QuestionAnsweringModelOutput, BertForQuestionAnswering, BaseModelOutputWithPoolingAndCrossAttentions
import math
from typing import List, Optional, Tuple, Union
import warnings


# Creatinf the Encoder Only model to read the weights from bert_base_encased.pth 
'''
Example of a single Layer of Bert Model:
encoder.layer.0.attention.self.query.weight torch.Size([768, 768])
encoder.layer.0.attention.self.query.bias torch.Size([768])
encoder.layer.0.attention.self.key.weight torch.Size([768, 768])
encoder.layer.0.attention.self.key.bias torch.Size([768])
encoder.layer.0.attention.self.value.weight torch.Size([768, 768])
encoder.layer.0.attention.self.value.bias torch.Size([768])
encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])
encoder.layer.0.attention.output.dense.bias torch.Size([768])
encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])
encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])
encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])
encoder.layer.0.intermediate.dense.bias torch.Size([3072])
encoder.layer.0.output.dense.weight torch.Size([768, 3072])
encoder.layer.0.output.dense.bias torch.Size([768])
encoder.layer.0.output.LayerNorm.weight torch.Size([768])
encoder.layer.0.output.LayerNorm.bias torch.Size([768])
'''
# Attention

'''
BertModel - > RegBert
BertLayer - > Block 
BertEncoder -> RegBertEncoder

'''


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor





# class Attention(BertSelfAttention):
#     def __init__(self, config, position_embedding_type=None):
#         super().__init__(config, position_embedding_type=position_embedding_type)
#         self.dropout_prob = config.attention_probs_dropout_prob
#         self.require_contiguous_qkv = True

#     # Func to get NxN attention map.
#     def get_NxN_attn_map(self, query, key, value, attn_mask=None, dropout_p=0.0,
#             is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:
#         L, S = query.size(-2), key.size(-2)
#         scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale
#         attn_bias = torch.zeros(L, S, dtype=query.dtype)
#         if is_causal:
#             assert attn_mask is None
#             temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
#             attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
#             attn_bias.to(query.dtype)

#         if attn_mask is not None:
#             if attn_mask.dtype == torch.bool:
#                 attn_bias.masked_fill_(attn_mask.logical_not(), float("-inf"))
#             else:
#                 print(query.shape, key.shape, attn_mask.shape, attn_bias.shape)
#                 attn_bias += attn_mask

#         if enable_gqa:
#             key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)
#             value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)

#         attn_weight = query @ key.transpose(-2, -1) * scale_factor
#         attn_weight += attn_bias
#         attn_weight = torch.softmax(attn_weight, dim=-1)
#         attn_weight = torch.dropout(attn_weight, dropout_p, train=True)
#         return attn_weight

#     # Adapted from BertSelfAttention
#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         attention_mask: Optional[torch.Tensor] = None,
#         head_mask: Optional[torch.FloatTensor] = None,
#         encoder_hidden_states: Optional[torch.FloatTensor] = None,
#         encoder_attention_mask: Optional[torch.FloatTensor] = None,
#         past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
#         output_attentions: Optional[bool] = False,
#     ) -> Tuple[torch.Tensor]:
#         if self.position_embedding_type != "absolute" or output_attentions or head_mask is not None:
#             # TODO: Improve this warning with e.g. `model.config._attn_implementation = "manual"` once implemented.
#             # logger.warning_once(
#             #     "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support "
#             #     "non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to "
#             #     "the manual attention implementation, but specifying the manual implementation will be required from "
#             #     "Transformers version v5.0.0 onwards. This warning can be removed using the argument "
#             #     '`attn_implementation="eager"` when loading the model.'
#             # )
#             return super().forward(
#                 hidden_states,
#                 attention_mask,
#                 head_mask,
#                 encoder_hidden_states,
#                 encoder_attention_mask,
#                 past_key_value,
#                 output_attentions,
#             )

#         bsz, tgt_len, _ = hidden_states.size()
#         print("Updated Class")

#         query_layer = self.transpose_for_scores(self.query(hidden_states))

#         # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention
#         # mask needs to be such that the encoder's padding tokens are not attended to.
#         is_cross_attention = encoder_hidden_states is not None

#         current_states = encoder_hidden_states if is_cross_attention else hidden_states
#         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask

#         # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning
#         if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:
#             key_layer, value_layer = past_key_value
#         else:
#             key_layer = self.transpose_for_scores(self.key(current_states))
#             value_layer = self.transpose_for_scores(self.value(current_states))
#             if past_key_value is not None and not is_cross_attention:
#                 key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
#                 value_layer = torch.cat([past_key_value[1], value_layer], dim=2)

#         if self.is_decoder:
#             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
#             # Further calls to cross_attention layer can then reuse all cross-attention
#             # key/value_states (first "if" case)
#             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
#             # all previous decoder key/value_states. Further calls to uni-directional self-attention
#             # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
#             # if encoder bi-directional self-attention `past_key_value` is always `None`
#             past_key_value = (key_layer, value_layer)

#         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom
#         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.
#         # Reference: https://github.com/pytorch/pytorch/issues/112577
#         if self.require_contiguous_qkv and query_layer.device.type == "cuda" and attention_mask is not None:
#             query_layer = query_layer.contiguous()
#             key_layer = key_layer.contiguous()
#             value_layer = value_layer.contiguous()

#         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment
#         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.
#         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create
#         # a causal mask in case tgt_len == 1.
#         is_causal = (
#             True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False
#         )


#         attn_map = self.get_NxN_attn_map(
#             query_layer,
#             key_layer,
#             value_layer,
#             attn_mask=attention_mask,
#             dropout_p=self.dropout_prob if self.training else 0.0,
#             is_causal=is_causal,
#         )

#         torch.save(attn_map, 'attn_map')

#         attn_output = attn_map @ value_layer

#         # attn_output = torch.nn.functional.scaled_dot_product_attention(
#         #     query_layer,
#         #     key_layer,
#         #     value_layer,
#         #     attn_mask=attention_mask,
#         #     dropout_p=self.dropout_prob if self.training else 0.0,
#         #     is_causal=is_causal,
#         # )

#         attn_output = attn_output.transpose(1, 2)
#         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)

#         outputs = (attn_output,)
#         if self.is_decoder:
#             outputs = outputs + (past_key_value,)
#         return outputs



# class RelProp(nn.Module):
#     def __init__(self):
#         super(RelProp, self).__init__()
#         # if not self.training:
#         self.register_forward_hook(forward_hook)
#     def gradprop(self, Z, X, S):
#         C = torch.autograd.grad(Z, X, S, retain_graph=True)
#         return C
#     def relprop(self, R, alpha):
#         return R
    
# class Clone(RelProp):
#     def forward(self, input, num):
#         self.__setattr__('num', num)
#         outputs = []
#         for _ in range(num):
#             outputs.append(input)
#         return outputs
#     def relprop(self, R, alpha):
#         Z = []
#         for _ in range(self.num):
#             Z.append(self.X)
#         S = [safe_divide(r, z) for r, z in zip(R, Z)]
#         C = self.gradprop(Z, self.X, S)[0]
#         R = self.X * C
#         return R

''' 
class Attention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob) #Dropout(config.attention_probs_dropout_prob)

        # self.matmul1 = MatMul()
        # self.matmul2 = MatMul()
        # self.softmax = Softmax(dim=-1)
        # self.add = Add()
        # self.mul = Mul()
        self.head_mask = None
        self.attention_mask = None
        # self.clone = Clone()

        self.attn_cam = None
        self.attn = None
        self.attn_gradients = None

    def get_attn(self):
        return self.attn

    def save_attn(self, attn):
        self.attn = attn

    def save_attn_cam(self, cam):
        self.attn_cam = cam

    def get_attn_cam(self):
        return self.attn_cam

    def save_attn_gradients(self, attn_gradients):
        self.attn_gradients = attn_gradients

    def get_attn_gradients(self):
        return self.attn_gradients

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def transpose_for_scores_relprop(self, x):
        return x.permute(0, 2, 1, 3).flatten(2)

    def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.FloatTensor] = None,
            head_mask: Optional[torch.FloatTensor] = None,
            encoder_hidden_states: Optional[torch.FloatTensor] = None,
            encoder_attention_mask: Optional[torch.FloatTensor] = None,
            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
            output_attentions: Optional[bool] = False,
            ):
        self.head_mask = head_mask
        self.attention_mask = attention_mask

        # h1, h2, h3 = self.clone(hidden_states, 3)
        # print("Attending")
        h1, h2, h3 = [hidden_states, hidden_states, hidden_states]
        
        mixed_query_layer = self.query(h1)

        # If this is instantiated as a cross-attention module, the keys
        # and values come from an encoder; the attention mask needs to be
        # such that the encoder's padding tokens are not attended to.
        if encoder_hidden_states is not None:
            mixed_key_layer = self.key(encoder_hidden_states)
            mixed_value_layer = self.value(encoder_hidden_states)
            attention_mask = encoder_attention_mask
        else:
            mixed_key_layer = self.key(h2)
            mixed_value_layer = self.value(h3)

        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        # attention_scores = self.matmul1([query_layer, key_layer.transpose(-1, -2)])
        attention_scores = torch.matmul([query_layer, key_layer.transpose(-1, -2)])
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
            # attention_scores = self.add([attention_scores, attention_mask])
            attention_scores = torch.add([attention_scores, attention_mask])

        # Normalize the attention scores to probabilities.
        # attention_probs = self.softmax(attention_scores)
        attention_probs = nn.Softmax(attention_scores)




        self.save_attn(attention_probs)
        attention_probs.register_hook(self.save_attn_gradients)



        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        # attention_probs = self.dropout(attention_probs)
        attention_probs = nn.Dropout(attention_probs)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        # context_layer = self.matmul2([attention_probs, value_layer])
        context_layer = torch.matmul([attention_probs, value_layer])

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
        return outputs





    def relprop(self, cam, **kwargs):
        # Assume output_attentions == False
        cam = self.transpose_for_scores(cam)

        # [attention_probs, value_layer]
        (cam1, cam2) = self.matmul2.relprop(cam, **kwargs)
        cam1 /= 2
        cam2 /= 2
        if self.head_mask is not None:
            # [attention_probs, head_mask]
            (cam1, _)= self.mul.relprop(cam1, **kwargs)


        self.save_attn_cam(cam1)

        cam1 = self.dropout.relprop(cam1, **kwargs)

        cam1 = self.softmax.relprop(cam1, **kwargs)

        if self.attention_mask is not None:
            # [attention_scores, attention_mask]
            (cam1, _) = self.add.relprop(cam1, **kwargs)

        # [query_layer, key_layer.transpose(-1, -2)]
        (cam1_1, cam1_2) = self.matmul1.relprop(cam1, **kwargs)
        cam1_1 /= 2
        cam1_2 /= 2

        # query
        cam1_1 = self.transpose_for_scores_relprop(cam1_1)
        cam1_1 = self.query.relprop(cam1_1, **kwargs)

        # key
        cam1_2 = self.transpose_for_scores_relprop(cam1_2.transpose(-1, -2))
        cam1_2 = self.key.relprop(cam1_2, **kwargs)

        # value
        cam2 = self.transpose_for_scores_relprop(cam2)
        cam2 = self.value.relprop(cam2, **kwargs)

        cam = self.clone.relprop((cam1_1, cam1_2, cam2), **kwargs)

        return cam
'''









class RegBert(BertModel):
    def __init__(self, config, num_registers=50, dev='mps'):
        
        super().__init__(config)
        self.num_registers = num_registers
        self.config = config
        if num_registers > 0: 
            self.reg_tokens = nn.Parameter(torch.zeros(1, num_registers, 768))
            self.reg_pos = nn.Parameter(torch.zeros(1, num_registers, 768))
        self.dev = dev
        trunc_normal_(self.reg_tokens, std=.02)
        trunc_normal_(self.reg_pos, std=.02)
        # self.init_reg_weights()
        
    
    def init_reg_weights(self):
        '''
        self.reg_encoder <- self.encoder(bert Model)
        self.reg_layer <- self.encoder.layer(bert Layer)
        self.reg_attention <- self.encoder.layer.attention(bert Attention)
        '''
        for i, l in enumerate(self.encoder.layer):
            new_attention = Attention(self.config)
            # l2 = {}
            # for k,v in l.attention.state_dict().items():
            #     k1 = k.replace('self.', '')
            #     l2[k1] = v
            # new_attention.load_state_dict(l2)
            new_attention.load_state_dict(l.attention.self.state_dict())
            l.attention.self = new_attention


    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        # past_key_values: Optional[List[torch.FloatTensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None):

        ones = torch.ones((attention_mask.shape[0], self.num_registers)).to(self.dev)
        attention_mask = torch.cat((ones, attention_mask), dim =1)
        batch_size, seq_length = input_ids.size()
        input_shape = input_ids.size()
        if token_type_ids is None:
            if hasattr(self.embeddings, "token_type_ids"):
                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]
                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)
                token_type_ids = buffered_token_type_ids_expanded
            else:
                token_type_ids = torch.zeros(input_shape, dtype=torch.long, dev=self.dev)
        
        # Here are the positional embeddings + word embeddings + token type embeddings
        input_embeds = self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
        # print("Forwarding")
        if self.num_registers > 0: 
            register = self.reg_tokens.expand(batch_size, -1, -1)
            register = torch.add(register, self.reg_pos)
            embedding_output = torch.cat((register, input_embeds), dim=1)
        else:
            embedding_output = input_embeds

        # use_sdpa_attention_masks = (
        #     self.attn_implementation == "sdpa"
        #     and self.position_embedding_type == "absolute"
        #     and head_mask is None
        #     and not output_attentions
        # ) 

        #prepare 4D attention mask if needed
        def prepare_mask(mask, dtype, target_length):
            batch, key_length = mask.shape 
            target_length = target_length if target_length is not None else key_length
            expanded_mask = mask[:, None, None, :].expand(batch, 1, target_length, key_length).to(self.dev)
            inverted_mask = 1.0 - expanded_mask
            return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)
        extended_attention_mask = prepare_mask(attention_mask, embedding_output.dtype, seq_length+self.num_registers)
        # print(extended_attention_mask.shape)
        encoder_extended_attention_mask = None
        head_mask = None


        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            # past_key_values=past_key_values,
            # use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None
        # print(f"Return Dict: {return_dict}, seq = {sequence_output.shape}, pooled: {pooled_output.shape}")
        # if not return_dict:
        #     return (sequence_output, pooled_output) + encoder_outputs[self.num_registers:]

        return BaseModelOutputWithPoolingAndCrossAttentions(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            # past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
            # cross_attentions=encoder_outputs.cross_attentions,
        )
        # return (
        #     sequence_output,
        #     pooled_output,
        #     encoder_outputs.past_key_values,
        #     encoder_outputs.hidden_states,
        #     encoder_outputs.attentions,
        #     encoder_outputs.cross_attentions,
        # )

        


class RegBertForQA(BertForQuestionAnswering):

    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        # self.bert = RegBert(config)
        self.bert = RegBert.from_pretrained('bert-base-uncased')
        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init() ####### Do we need this??

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        start_positions: Optional[torch.Tensor] = None,
        end_positions: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.Tensor]]:
        r"""
        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the start of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
            are not taken into account for computing the loss.
        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the end of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
            are not taken into account for computing the loss.
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        # print('outputs[0]',outputs[0].shape)

        sequence_output = outputs[0]

        logits = self.qa_outputs(sequence_output)
        # print('logits: ',logits.shape)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        start_logits = start_logits[:, self.bert.num_registers:]
        end_logits = end_logits[:, self.bert.num_registers:]

        # print('start_logits: ',start_logits.shape)
        # print('end_logits: ', end_logits.shape)
        # print('start_positions: ',start_positions.shape)

        total_loss = None
        if start_positions is not None and end_positions is not None:
            # If we are on multi-GPU, split add a dimension
            if len(start_positions.size()) > 1:
                start_positions = start_positions.squeeze(-1)
            if len(end_positions.size()) > 1:
                end_positions = end_positions.squeeze(-1)
            # sometimes the start/end positions are outside our model inputs, we ignore these terms
            ignored_index = start_logits.size(1)
            start_positions = start_positions.clamp(0, ignored_index)
            end_positions = end_positions.clamp(0, ignored_index)

            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2

        if not return_dict:
            output = (start_logits, end_logits) + outputs[2:]
            return ((total_loss,) + output) if total_loss is not None else output

        return QuestionAnsweringModelOutput(
            loss=total_loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )




    
            






    
# model = RegBert.from_pretrained('bert-base-uncased')
# model.init_reg_weights()
# print(model.config)
# for name, value in model.state_dict().items():
#     print(name, value.shape)


# Block
# Feed Forward